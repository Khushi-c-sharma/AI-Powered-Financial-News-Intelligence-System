"""
Deduplication Agent (Optimized - FAISS + SentenceTransformer Embeddings)
------------------------------------------------------------------------

This service:
- Loads persisted FAISS index + ID mapping generated by the Embedding Agent.
- Loads article metadata + embeddings from vectors.db.
- Performs semantic deduplication using cosine similarity.
- Groups articles into clusters of near-duplicates.
- Writes dedup clusters into clusters.db.
- Supports incremental deduplication (only new articles).
- Works fully async with FastAPI + SQLite.

Run:
uvicorn agents.dedup_agent:app --reload --port 8003

Powershell Test:
Invoke-RestMethod -Method Post http://localhost:8003/dedup/run

"""

import uuid
import json
import logging
import numpy as np
from pathlib import Path
from typing import List, Dict, Optional, Set
from contextlib import asynccontextmanager
from datetime import datetime, timezone

import faiss
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from sqlalchemy.pool import StaticPool

from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker, declarative_base
from sqlalchemy import (
    Column, String, Text, TIMESTAMP, Float, select, func, Index, LargeBinary
)

# ---------------------------------------------------------
# Logging
# ---------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - DEDUP - %(levelname)s - %(message)s"
)
logger = logging.getLogger("DedupAgent")

Base = declarative_base()

# ---------------------------------------------------------
# PATH SETUP
# ---------------------------------------------------------
PROJECT_ROOT = Path(__file__).resolve().parents[1]
DATA_DIR = PROJECT_ROOT / "data"

INGEST_DB_PATH = DATA_DIR / "ingestion" / "raw_articles.db"
EMBED_DB_PATH = DATA_DIR / "embeddings" / "vectors.db"
DEDUP_DB_PATH = DATA_DIR / "dedup" / "clusters.db"

# Create dedup directory
(DATA_DIR / "dedup").mkdir(parents=True, exist_ok=True)

FAISS_INDEX_PATH = DATA_DIR / "embeddings" / "faiss.index"
FAISS_MAP_PATH = DATA_DIR / "embeddings" / "faiss_id_map.json"

# ---------------------------------------------------------
# SQLite Engines with connection pooling
# ---------------------------------------------------------
ingest_engine = create_async_engine(
    f"sqlite+aiosqlite:///{INGEST_DB_PATH}",
    echo=False,
    connect_args={"check_same_thread": False},
    poolclass=StaticPool
)
emb_engine = create_async_engine(
    f"sqlite+aiosqlite:///{EMBED_DB_PATH}",
    echo=False,
    connect_args={"check_same_thread": False},
    poolclass=StaticPool
)
dedup_engine = create_async_engine(
    f"sqlite+aiosqlite:///{DEDUP_DB_PATH}",
    echo=False,
    connect_args={"check_same_thread": False},
    poolclass=StaticPool
)

IngestSession = sessionmaker(ingest_engine, class_=AsyncSession, expire_on_commit=False)
EmbSession = sessionmaker(emb_engine, class_=AsyncSession, expire_on_commit=False)
DedupSession = sessionmaker(dedup_engine, class_=AsyncSession, expire_on_commit=False)

def now():
    return datetime.now(timezone.utc)

# ---------------------------------------------------------
# Database Models
# ---------------------------------------------------------
class RawArticle(Base):
    """Minimal article metadata for dedup cluster output."""
    __tablename__ = "raw_articles"

    id = Column(String, primary_key=True)
    article_id = Column(String, unique=True)
    title = Column(Text)
    source = Column(String)
    published_date = Column(TIMESTAMP(timezone=True))


class ArticleEmbedding(Base):
    """Embedding vectors generated by the Embedding Agent."""
    __tablename__ = "article_embeddings"

    id = Column(String, primary_key=True)
    article_id = Column(String, unique=True, index=True)
    vector = Column(LargeBinary)  # Changed from Text to LargeBinary
    model = Column(String)
    created_at = Column(TIMESTAMP(timezone=True))


class StoryCluster(Base):
    """
    Cluster entry for deduplicated stories.
    
    - cluster_id groups articles into the same dedup group
    - label: canonical | duplicate
    - similarity: cosine similarity to canonical
    """
    __tablename__ = "story_clusters"
    __table_args__ = (
        Index('idx_cluster_article', 'cluster_id', 'article_id'),
        Index('idx_article_label', 'article_id', 'label'),
    )

    id = Column(String, primary_key=True)
    cluster_id = Column(String, index=True)
    article_id = Column(String, index=True)
    similarity = Column(Float)
    label = Column(String)  # canonical | duplicate
    canonical_article_id = Column(String, index=True)  # Reference to canonical article
    created_at = Column(TIMESTAMP(timezone=True), default=now)


class DeduplicationRun(Base):
    """Track deduplication runs for incremental processing."""
    __tablename__ = "dedup_runs"
    
    id = Column(String, primary_key=True)
    run_at = Column(TIMESTAMP(timezone=True), default=now)
    articles_processed = Column(Float)
    clusters_created = Column(Float)
    duplicates_found = Column(Float)

# ---------------------------------------------------------
# Load Persistent FAISS Index
# ---------------------------------------------------------
faiss_index = None
faiss_id_map: Dict[int, str] = {}

def load_faiss():
    """Load FAISS index and ID map created by the embedding agent."""
    global faiss_index, faiss_id_map

    if not FAISS_INDEX_PATH.exists():
        raise RuntimeError(
            "FAISS index file not found. Run Embedding Agent first.\n"
            f"Expected path: {FAISS_INDEX_PATH}"
        )

    faiss_index = faiss.read_index(str(FAISS_INDEX_PATH))
    logger.info(f"Loaded FAISS index with {faiss_index.ntotal} vectors")

    if not FAISS_MAP_PATH.exists():
        raise RuntimeError(
            "FAISS ID map not found. Ensure Embedding Agent saved it.\n"
            f"Expected path: {FAISS_MAP_PATH}"
        )

    with open(FAISS_MAP_PATH) as f:
        raw_map = json.load(f)
        faiss_id_map = {int(i): aid for i, aid in raw_map.items()}
    
    logger.info(f"Loaded {len(faiss_id_map)} FAISS ID mappings")

    # Validate consistency
    if faiss_index.ntotal != len(faiss_id_map):
        logger.error(
            f"FAISS index has {faiss_index.ntotal} vectors but "
            f"ID map has {len(faiss_id_map)} entries. Rebuild recommended."
        )

# ---------------------------------------------------------
# Similarity Function (Vectorized)
# ---------------------------------------------------------
def cosine_similarity_batch(query: np.ndarray, vectors: np.ndarray) -> np.ndarray:
    """
    Compute cosine similarity between query vector and batch of vectors.
    More efficient than computing one-by-one.
    """
    # Normalize vectors
    query_norm = query / (np.linalg.norm(query) + 1e-8)
    vectors_norm = vectors / (np.linalg.norm(vectors, axis=1, keepdims=True) + 1e-8)
    
    # Compute dot product
    similarities = np.dot(vectors_norm, query_norm)
    return similarities

# ---------------------------------------------------------
# Deduplication Logic (Optimized)
# ---------------------------------------------------------
SIM_THRESHOLD = 0.88  # Works well with MiniLM embeddings
TOP_K_NEIGHBORS = 50  # Check more neighbors for better recall

class DeduplicationStats(BaseModel):
    clusters: int
    articles_processed: int
    duplicates_found: int
    already_clustered: int
    processing_time_seconds: float

async def run_deduplication(
    force_rebuild: bool = False,
    similarity_threshold: float = SIM_THRESHOLD
) -> DeduplicationStats:
    """
    Load embeddings, search for near duplicates, create clusters.
    
    Args:
        force_rebuild: If True, clear existing clusters and rebuild from scratch
        similarity_threshold: Minimum cosine similarity to consider as duplicate
    
    Returns:
        DeduplicationStats with processing results
    """
    import time
    start_time = time.time()

    async with EmbSession() as emb_db, DedupSession() as dedup_db:
        
        # Clear existing clusters if force rebuild
        if force_rebuild:
            await dedup_db.execute(StoryCluster.__table__.delete())
            await dedup_db.commit()
            logger.info("Cleared existing clusters for rebuild")
        
        # Get already clustered articles for incremental dedup
        already_clustered: Set[str] = set()
        if not force_rebuild:
            result = await dedup_db.execute(
                select(StoryCluster.article_id).distinct()
            )
            already_clustered = {row[0] for row in result.all()}
            logger.info(f"Found {len(already_clustered)} already clustered articles")

        # Load all embeddings
        res = await emb_db.execute(select(ArticleEmbedding))
        rows = res.scalars().all()

        if not rows:
            return DeduplicationStats(
                clusters=0,
                articles_processed=0,
                duplicates_found=0,
                already_clustered=0,
                processing_time_seconds=0.0
            )

        # Build article_id to index mapping
        article_to_idx: Dict[str, int] = {}
        idx_to_article: Dict[int, str] = {}
        vectors_list = []
        
        for idx, r in enumerate(rows):
            article_id = r.article_id
            
            # Skip if already clustered in incremental mode
            if not force_rebuild and article_id in already_clustered:
                continue
            
            vec = np.frombuffer(r.vector, dtype=np.float32)
            vectors_list.append(vec)
            article_to_idx[article_id] = idx
            idx_to_article[idx] = article_id

        if not vectors_list:
            logger.info("No new articles to cluster")
            return DeduplicationStats(
                clusters=0,
                articles_processed=0,
                duplicates_found=0,
                already_clustered=len(already_clustered),
                processing_time_seconds=time.time() - start_time
            )

        vectors = np.vstack(vectors_list)
        logger.info(f"Processing {len(vectors)} articles for deduplication")

        # Ensure FAISS matches expectations
        if faiss_index.ntotal == 0:
            logger.error("FAISS index is empty!")
            raise RuntimeError("FAISS index has no vectors")

        visited = set()
        cluster_id_count = 1
        cluster_records = []
        duplicates_found = 0

        # Process each article
        for i in range(len(vectors)):
            if i in visited:
                continue

            canonical_article = idx_to_article[i]
            cluster_id = f"cluster_{uuid.uuid4().hex[:8]}"  # Unique cluster IDs
            
            visited.add(i)

            # Add canonical entry
            cluster_records.append({
                "cluster_id": cluster_id,
                "article_id": canonical_article,
                "canonical_article_id": canonical_article,
                "similarity": 1.0,
                "label": "canonical"
            })

            # FAISS k-NN search for similar articles
            query_vec = vectors[i:i+1]
            distances, indices = faiss_index.search(query_vec, TOP_K_NEIGHBORS)

            # Convert L2 distances to similarities and find duplicates
            found_duplicates = []
            
            for dist, faiss_idx in zip(distances[0], indices[0]):
                if faiss_idx == -1:  # Invalid index
                    continue
                
                # Get article_id from FAISS index
                candidate_article = faiss_id_map.get(int(faiss_idx))
                if not candidate_article or candidate_article == canonical_article:
                    continue
                
                # Get local index if this article is in our processing batch
                candidate_idx = article_to_idx.get(candidate_article)
                if candidate_idx is None or candidate_idx in visited:
                    continue
                
                # Calculate cosine similarity
                sim = float(np.dot(vectors[i], vectors[candidate_idx]) / 
                           (np.linalg.norm(vectors[i]) * np.linalg.norm(vectors[candidate_idx])))

                if sim >= similarity_threshold:
                    visited.add(candidate_idx)
                    found_duplicates.append({
                        "cluster_id": cluster_id,
                        "article_id": candidate_article,
                        "canonical_article_id": canonical_article,
                        "similarity": sim,
                        "label": "duplicate"
                    })
                    duplicates_found += 1

            # Add all duplicates for this cluster
            cluster_records.extend(found_duplicates)
            
            if found_duplicates:
                logger.info(
                    f"Cluster {cluster_id}: found {len(found_duplicates)} "
                    f"duplicates for {canonical_article}"
                )
            
            cluster_id_count += 1

        # Batch insert to DB (more efficient)
        logger.info(f"Saving {len(cluster_records)} cluster records to database...")
        
        for r in cluster_records:
            dedup_db.add(StoryCluster(
                id=str(uuid.uuid4()),
                cluster_id=r["cluster_id"],
                article_id=r["article_id"],
                canonical_article_id=r["canonical_article_id"],
                similarity=r["similarity"],
                label=r["label"]
            ))

        # Record this deduplication run
        dedup_db.add(DeduplicationRun(
            id=str(uuid.uuid4()),
            articles_processed=len(vectors),
            clusters_created=cluster_id_count - 1,
            duplicates_found=duplicates_found
        ))

        await dedup_db.commit()
        
        processing_time = time.time() - start_time
        logger.info(f"Deduplication completed in {processing_time:.2f}s")

        return DeduplicationStats(
            clusters=cluster_id_count - 1,
            articles_processed=len(vectors),
            duplicates_found=duplicates_found,
            already_clustered=len(already_clustered),
            processing_time_seconds=processing_time
        )

# ---------------------------------------------------------
# FastAPI Lifespan
# ---------------------------------------------------------
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Initialize DB and load FAISS on startup."""
    async with dedup_engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)
    logger.info("Deduplication DB initialized")
    
    try:
        load_faiss()
    except RuntimeError as e:
        logger.error(f"Failed to load FAISS: {e}")
        logger.warning("Dedup service will not work without FAISS index")
    
    yield

app = FastAPI(
    title="Deduplication Agent (Optimized)",
    version="2.0.0",
    lifespan=lifespan
)

# ---------------------------------------------------------
# API ENDPOINTS
# ---------------------------------------------------------

@app.post("/dedup/run", response_model=DeduplicationStats)
async def run(
    force_rebuild: bool = False,
    similarity_threshold: float = SIM_THRESHOLD
):
    """
    Run semantic deduplication across embedded articles.
    
    Args:
        force_rebuild: Clear existing clusters and rebuild from scratch
        similarity_threshold: Minimum cosine similarity (0.0-1.0) for duplicates
    """
    if faiss_index is None:
        raise HTTPException(
            status_code=503,
            detail="FAISS index not loaded. Ensure Embedding Agent has run."
        )
    
    result = await run_deduplication(force_rebuild, similarity_threshold)
    return result


@app.get("/dedup/clusters")
async def get_clusters(limit: int = 20, cluster_id: Optional[str] = None):
    """
    Return clusters for inspection.
    
    Args:
        limit: Maximum number of records to return
        cluster_id: If provided, return only this specific cluster
    """
    async with DedupSession() as db:
        query = select(StoryCluster)
        
        if cluster_id:
            query = query.where(StoryCluster.cluster_id == cluster_id)
        
        query = query.limit(limit)
        res = await db.execute(query)
        rows = res.scalars().all()

        return {
            "total": len(rows),
            "clusters": [
                {
                    "cluster_id": r.cluster_id,
                    "article_id": r.article_id,
                    "canonical_article_id": r.canonical_article_id,
                    "similarity": round(r.similarity, 4),
                    "label": r.label
                }
                for r in rows
            ]
        }


@app.get("/dedup/article/{article_id}")
async def article_cluster(article_id: str):
    """Return the cluster information for a specific article."""
    async with DedupSession() as db:
        res = await db.execute(
            select(StoryCluster).where(StoryCluster.article_id == article_id)
        )
        rows = res.scalars().all()

        if not rows:
            return {"message": "Article not found in any cluster"}

        cluster_info = rows[0]
        
        # Get all articles in the same cluster
        cluster_res = await db.execute(
            select(StoryCluster)
            .where(StoryCluster.cluster_id == cluster_info.cluster_id)
            .order_by(StoryCluster.similarity.desc())
        )
        cluster_articles = cluster_res.scalars().all()

        return {
            "article_id": article_id,
            "cluster_id": cluster_info.cluster_id,
            "label": cluster_info.label,
            "similarity": cluster_info.similarity,
            "canonical_article_id": cluster_info.canonical_article_id,
            "cluster_size": len(cluster_articles),
            "related_articles": [
                {
                    "article_id": a.article_id,
                    "similarity": round(a.similarity, 4),
                    "label": a.label
                }
                for a in cluster_articles
            ]
        }


@app.get("/dedup/stats")
async def stats():
    """Return deduplication statistics."""
    async with DedupSession() as db:
        total_clusters = await db.execute(
            select(func.count(StoryCluster.cluster_id.distinct()))
        )
        
        total_articles = await db.execute(
            select(func.count()).select_from(StoryCluster)
        )
        
        canonical_count = await db.execute(
            select(func.count())
            .select_from(StoryCluster)
            .where(StoryCluster.label == "canonical")
        )
        
        duplicate_count = await db.execute(
            select(func.count())
            .select_from(StoryCluster)
            .where(StoryCluster.label == "duplicate")
        )
        
        # Get last run info
        last_run_res = await db.execute(
            select(DeduplicationRun)
            .order_by(DeduplicationRun.run_at.desc())
            .limit(1)
        )
        last_run = last_run_res.scalar_one_or_none()

        return {
            "total_clusters": total_clusters.scalar() or 0,
            "total_entries": total_articles.scalar() or 0,
            "canonical_articles": canonical_count.scalar() or 0,
            "duplicate_articles": duplicate_count.scalar() or 0,
            "last_run": {
                "timestamp": last_run.run_at.isoformat() if last_run else None,
                "articles_processed": last_run.articles_processed if last_run else 0,
                "clusters_created": last_run.clusters_created if last_run else 0,
                "duplicates_found": last_run.duplicates_found if last_run else 0
            } if last_run else None
        }


@app.delete("/dedup/clear")
async def clear_clusters():
    """Clear all deduplication clusters. Use with caution!"""
    async with DedupSession() as db:
        await db.execute(StoryCluster.__table__.delete())
        await db.execute(DeduplicationRun.__table__.delete())
        await db.commit()
    
    return {"message": "All clusters cleared successfully"}


@app.get("/health")
async def health():
    """Health check endpoint."""
    return {
        "status": "healthy" if faiss_index is not None else "degraded",
        "faiss_index_loaded": faiss_index is not None,
        "faiss_index_size": faiss_index.ntotal if faiss_index else 0,
        "id_map_entries": len(faiss_id_map)
    }


# ---------------------------------------------------------
# Run server
# ---------------------------------------------------------
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8003)